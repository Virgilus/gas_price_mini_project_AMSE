{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Project 2018-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Collect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture du fichier YAML (parameters.yaml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Par défaut le fichier YAML télécharge les données de 2015 à 2018.\n",
    "On définit une fonction de vérification des données chargées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Cette fonction vérifie que les années sont correctement indiquées dans le fichier de configuration.######\n",
    "\n",
    "def verify_years(params):\n",
    "    \n",
    "    # On teste si first_year et last_year sont comprises entre 2007 et 2018.\n",
    "    # Si c'est correct, on renvoie True.\n",
    "    \n",
    "    if (2007 <= int(params['first_year']) <= 2018 \n",
    "        and 2007 <= int(params['last_year']) <= 2018 \n",
    "        and int(params['first_year']) <= int(params['last_year'])): return True\n",
    "               \n",
    "    # Dans le cas contraire, on envoie un message d'erreur\n",
    "    else:\n",
    "        print (\"Erreur dans le fichier YAML, last_year et/ou first_year n'est pas compris entre 2007 et 2018 ou bien first_year est supérieur à last_year\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ouverture et chargement des paramètres du fichiel YALM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('parameters.yml', 'r') as file: # On ouvre le fichier YAML\n",
    "    params = yaml.load(file) # On créé un dictionnaire de paramètres à partir des données du fichier YAML\n",
    "\n",
    "\n",
    "types_files = ['Prix', 'Stations','Services'] # On définit les types de fichiers à télécharger\n",
    "files_to_dl = [] # On créé une liste vide qui accueillera les noms des fichiers\n",
    "\n",
    "if verify_years(params) == True: # Si les années précisées sont correctes\n",
    "    \n",
    "     # On créé une liste contenant les années avec la fonction range : \n",
    "    years = list((range(int(params['first_year']), int(params['last_year'])+1)))\n",
    "    \n",
    "    for year in years: # Et pour chaque année\n",
    "        for each_type in types_files: # et chaque type de fichiers\n",
    "            \n",
    "            # On le rajoute à la liste de fichiers à télécharger (avec l'extension zip).\n",
    "            files_to_dl.append(each_type+str(year)+'.zip')\n",
    "\n",
    "# Comme le fichier prix2018 porte un nom différent sur le github, on rectifie ceci.\n",
    "for index, item in enumerate(files_to_dl):\n",
    "    if 'Prix2018.zip' in item:\n",
    "        files_to_dl[index] = 'Prix20181114.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Téléchargement des fichiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fichiers se téléchargent par défaut dans un répertoire nommé \"data\" là où est executé le script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL du répertoire à distance :\n",
    "# (normalement https://raw.githubusercontent.com/rvm-courses/GasPrices/master/)\n",
    "url = params['url']\n",
    "\n",
    "# Repertoire local éventuel où télécharger les fichiers\n",
    "# (S'il n'existe pas, il sera créé)\n",
    "local_dir = 'data/'\n",
    "\n",
    "\n",
    "# On vérifie si le répertoire de destination existe\n",
    "if local_dir: # Si la string n'est pas vide\n",
    "    if not os.path.exists(local_dir): # On teste si le répertoire n'existe pas déjà\n",
    "        os.makedirs(local_dir) # Et si c'est le cas on en crée un\n",
    "\n",
    "count_files = 0 # On crée une variable qui comptera le nombre de fichiers téléchargés.\n",
    "\n",
    "# Pour chacun des fichiers de la liste :\n",
    "\n",
    "for name in files_to_dl:\n",
    "    \n",
    "    r = requests.head(url+name) # On établit la connexion sans télécharger\n",
    "    if r.status_code == 200: # On vérifie si le fichier existe\n",
    "        \n",
    "        print(\"Fichier actuellement téléchargé :\", name)\n",
    "        r = requests.get(url+name) # On télécharge le fichier\n",
    "\n",
    "        with open(local_dir+name, \"wb\") as file: # On ouvre le fichier en écriture\n",
    "            file.write(r.content) # On l'écrit sur le disque dur\n",
    "            zip_ref = zipfile.ZipFile(local_dir+name) # On créé un objet zip\n",
    "            zip_ref.extractall(local_dir) # on extrait les fichiers vers le répertoire\n",
    "            zip_ref.close() # on ferme la référence au fichier zippé\n",
    "            file.close() # On ferme également le fichier (sans cette ligne certains OS renvoient une erreur)\n",
    "            os.remove(local_dir+name) # on efface le fichier zip\n",
    "            \n",
    "            count_files += 1 # on incrémente count_files\n",
    "\n",
    "print ('Le programme a téléchargé et dezippé avec succès', count_files, 'fichiers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Début de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "# Pour contrôler l'installation de Java\n",
    "#os.environ['JAVA_HOME']\n",
    "\n",
    "# Variables à modifier selon le poste sur lequel est executé le script.\n",
    "#os.environ['SPARK_HOME'] ='C:\\spark\\spark-2.4.0-bin-hadoop2.7'\n",
    "\n",
    "sc = pyspark.SparkContext() # On crée un Sparkcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlsc = pyspark.SQLContext(sc)\n",
    "spark = sqlsc.sparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation – Step 1 – Merging gas prices files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des fichiers contenant les relevés annuels des prix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "#Schéma adapté aux fichiers de type prix.csv\n",
    "\n",
    "schema_prix = StructType([ \n",
    "            StructField('id_station', StringType(), True),\n",
    "            StructField('code_postal', StringType(), True),\n",
    "            StructField('type_station', StringType(), True),\n",
    "            StructField('latitude', DoubleType(), True),\n",
    "            StructField('longitude', DoubleType(), True),\n",
    "            StructField('date_releve', DateType(), True),\n",
    "            StructField('type_carburant', IntegerType(), True),\n",
    "            StructField('libelle_carburant', StringType(), True),\n",
    "            StructField('prix', IntegerType(), True)])\n",
    "\n",
    "#On créé un ddf à partir de tous les CSV contenant le mot prix dans leur noms.\n",
    "\n",
    "prix_ddf = sqlsc.read.option('sep', ';').option('header', 'false').schema(schema_prix).csv('data/*Prix*.csv') #importation\n",
    "prix_ddf = prix_ddf.withColumn('prix', col('prix') / 1000) #On divise par 1000 la colonne prix.\n",
    "\n",
    "#display(prix_ddf) # On vérifie éventuellement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des fichiers contenant les informations relatives aux stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schéma adapté aux fichiers stations\n",
    "\n",
    "schema_stations = StructType([\n",
    "            StructField('id_station', StringType(), True),\n",
    "            StructField('code_postal', StringType(), True),\n",
    "            StructField('type_station', StringType(), True),\n",
    "            StructField('latitude', DoubleType(), True),\n",
    "            StructField('longitude', DoubleType(), True),\n",
    "            StructField('adresse', StringType(), True),\n",
    "            StructField('city', StringType(), True)])\n",
    "\n",
    "#On créé un ddf à partir de tous les CSV contenant le mot Stations dans leur noms.\n",
    "\n",
    "stations_ddf = sqlsc.read.option('sep', '|').option('header', 'false').schema(schema_stations).csv('data/*Stations*.csv') #importation\n",
    "\n",
    "#display(stations_ddf) # On vérifie éventuellement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enregistrement des DDF comme des tables SQL, traitement des dates et des latitudes et des longitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Fonction permettant de diviser par 10**5, et d'arrondir à 3 chiffres après la virgule les latitudes et les longitudes.\n",
    "\n",
    "def div_round(name):\n",
    "    return f.round((col(name) / 10**5),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Déclaration des DDF comme étant des tables SQL\n",
    "\n",
    "sqlsc.registerDataFrameAsTable(prix_ddf, 'prix_ddf')\n",
    "sqlsc.registerDataFrameAsTable(stations_ddf, 'stations_ddf')\n",
    "\n",
    "#prix_ddf.printSchema()# on vérifie éventuellement les schémas.\n",
    "#stations_ddf.printSchema()\n",
    "\n",
    "# On créé des nouvelles colonnes comportant l'année, le mois et la semaine.\n",
    "\n",
    "prix_ddf = sqlsc.sql(\n",
    "    'SELECT *, YEAR(date_releve) AS year, '\n",
    "    'MONTH(date_releve) AS month, '\n",
    "    'weekofyear(date_releve) AS week From prix_ddf'\n",
    ").cache()\n",
    "\n",
    "# On rectifie la lat et la lon et on arrondit le tout\n",
    "\n",
    "prix_ddf = prix_ddf.withColumn('latitude', div_round('latitude'))\n",
    "prix_ddf = prix_ddf.withColumn('longitude', div_round('longitude'))\n",
    "\n",
    "stations_ddf = stations_ddf.withColumn('latitude', div_round('latitude'))\n",
    "stations_ddf = stations_ddf.withColumn('longitude', div_round('longitude'))\n",
    "\n",
    "#stations_ddf.show(5) # Vérification éventuelle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation - Step 2 : Deduplicating stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des fichiers stations avec Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier stations étant de petite taille (inférieur à 40 000 lignes), on peut le traiter sous Pandas avant de le réexporter sous Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur certains ordinateurs la fonction toPandas() ne fonctionne pas. Dans ce cas on peut réimporter les fichiers stations à l'aide de ce code :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Inutile de d'exécuter cette cellule si Spark fonctionne correctement. Cellule en mode Raw.\n",
    "\n",
    "import glob\n",
    "\n",
    "files = glob.glob('data/*Stations*.csv') # On veut tous les fichiers contenant le mot Stations dans le titre\n",
    "\n",
    "#path = \"data/*Stations*.csv\" #Chemin du fichier.\n",
    "\n",
    "dtypes = {'id_station':str, 'code_postal':str, 'type_station':str, 'latitude':float, 'longitude':float, 'adresse':str, 'city':str}\n",
    "\n",
    "pd_stations_df = pd.DataFrame()\n",
    "\n",
    "for f in files:\n",
    "    data = pd.read_csv(f, sep='|', header=None, names=list(dtypes.keys()), dtype=dtypes)\n",
    "    pd_stations_df = pd_stations_df.append(data)\n",
    "    \n",
    "# pd_stations_df.count() # Vérification du nombre de lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On convertit le ddf spark en df Pandas\n",
    "\n",
    "pd_stations_df = spark.sql('SELECT * FROM stations_ddf').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déclaration des fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Élève à la bonne puissance (10**5), et arrondit au 4ème chiffre après la virgule les latitudes et les longitudes\n",
    "\n",
    "def div_round(name):\n",
    "    return round((pd_stations_df[name] / 10**5),3)\n",
    "\n",
    "\n",
    "##### Afin d'enlever les multiples erreurs et coquilles dans les différentes colonnes du df stations,\n",
    "##### on va lui faire subir une série d'opérations afin d'enlever les accents, de passer la chaine de caractère en minuscule \n",
    "##### d'enlever les double espaces, les caractères spéciaux ainsi qu'un certain nombre de noms de voies.\n",
    "##### Il sera ainsi bien plus facile de détecter les doublons.\n",
    "\n",
    "def clean_text(texte):\n",
    "    \n",
    "    if type(texte) == str: # Si c'est bien une string\n",
    "        \n",
    "        texte = unidecode.unidecode(texte) # On remplace les lettres accentuées par leurs versions sans accents\n",
    "        texte = texte.lower() # On met le tout en minuscule\n",
    "        texte = re.sub(' +', ' ', texte) # On enlève les double espaces, et on les remplace par un seul.\n",
    "        texte = re.sub('[^A-Za-z0-9\\s]+',' ', texte) #On enlève tout ce qui n'est pas alphanumérique, sauf les espaces\n",
    "        # On enlève la plupart des voies\n",
    "        texte = re.sub('\\\\b(rue|avenue|av|chemin|boulevard|bvd|promenade|impasse|route|allee|chez|cours|de|la|le|du|sur|dans|en)\\\\W', '' , texte)\n",
    "        texte = re.sub(' ', '', texte) # On enlève tous les espaces.\n",
    "        return texte #On retourne la nouvelle string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On nettoie les variables city et adresse grâce à la fonction vue précédemment. On traite aussi la latitude et la longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_stations_df['city'] = pd_stations_df['city'].apply(clean_text)\n",
    "pd_stations_df['adresse'] = pd_stations_df['adresse'].apply(clean_text)\n",
    "pd_stations_df['latitude'] = div_round('latitude')\n",
    "pd_stations_df['longitude'] = div_round('longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On commence par retirer les stations qui ont la même adresse dans la même ville.\n",
    "pd_stations_sans_doublons_df = pd_stations_df.drop_duplicates(subset=['adresse','city'], inplace=False)\n",
    "\n",
    "# Puis celles qui ont la même latitude et longitude\n",
    "pd_stations_sans_doublons_df = pd_stations_sans_doublons_df.drop_duplicates(subset=['latitude','longitude'], inplace=False)\n",
    "\n",
    "# A ce stade là il ne reste que peu de doublons potentiels lorsqu'on travaille sur les années 2015 - 2018\n",
    "# (ce qui correspond à la taille maximale que peut prendre le fichier station global).\n",
    "# Après examen manuel de ces doublons, on constate qu'en dépit du fait que leur seul point commun soit l'ID, il s'agit\n",
    "# en réalité des mêmes stations. On peut donc finalement filtrer par l'ID pour obtenir une liste sans doublons.\n",
    "pd_stations_sans_doublons_df = pd_stations_sans_doublons_df.drop_duplicates(subset=['id_station'], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ré-importation dans Spark du fichier stations, après avoir été traité sous Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut l'exporter en csv pour ensuite le réimporter sous Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_stations_sans_doublons_df.to_csv('data/station_ddf_clean.csv', index=False)\n",
    "\n",
    "#importation\n",
    "stations_ddf_clean = sqlsc.read.option('sep', '|').option('header', 'false').schema(schema_stations).csv('data/station_ddf_clean.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation - Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une table day_price qui contient le prix moyen par jour et par carburant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    'SELECT libelle_carburant, dayofyear(date_releve) as day, round(avg(prix), 3) as day_price '\n",
    "    'FROM prix_ddf '\n",
    "    'WHERE libelle_carburant is not null '\n",
    "    'GROUP BY libelle_carburant, dayofyear(date_releve)'\n",
    ").cache().createOrReplaceTempView('day_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une table day_price qui contient le prix moyen par jour et par carburant et par departement (cela servira à la représentation grpahique future)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    'SELECT libelle_carburant, dayofyear(date_releve) as day, round(avg(prix), 3) as daydep_price,SUBSTRING(code_postal, 0, 2) as departement '\n",
    "    'FROM prix_ddf '\n",
    "    'WHERE libelle_carburant is not null '\n",
    "    'GROUP BY libelle_carburant, dayofyear(date_releve), SUBSTRING(code_postal, 0, 2) '\n",
    ").cache().createOrReplaceTempView('daydep_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un indice de prix pour chaque station et par jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    'SELECT id_station, prix_ddf.libelle_carburant, dayofyear(date_releve) as day, 100*((avg(prix)-day_price.day_price) / day_price.day_price + 1) as indice '\n",
    "    'FROM prix_ddf, day_price '\n",
    "    'WHERE prix_ddf.libelle_carburant is not null '\n",
    "        'and prix_ddf.libelle_carburant = day_price.libelle_carburant and dayofyear(date_releve) = day_price.day '\n",
    "    'GROUP BY id_station, prix_ddf.libelle_carburant, dayofyear(date_releve), day_price.day_price'\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un indice de prix par jour et par département. (Pour la représentation graphique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    'SELECT id_station, prix_ddf.libelle_carburant, departement, dayofyear(date_releve) as day, 100*((avg(prix)-daydep_price.daydep_price) / daydep_price.daydep_price + 1) as indice '\n",
    "    'FROM prix_ddf, daydep_price '\n",
    "    'WHERE prix_ddf.libelle_carburant is not null '\n",
    "        'and prix_ddf.libelle_carburant = daydep_price.libelle_carburant and dayofyear(date_releve) = daydep_price.day '\n",
    "    'GROUP BY id_station, prix_ddf.libelle_carburant, dayofyear(date_releve), daydep_price.daydep_price, departement'\n",
    ").cache().createOrReplaceTempView('indice_dep1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization (1/2)\n",
    "### Graphique 1: Représentation de l'évolution journalière du prix moyen des carburants en France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_price_df=spark.sql(\n",
    "    'SELECT libelle_carburant, dayofyear(date_releve) as day, round(avg(prix), 3) as day_price '\n",
    "    'FROM prix_ddf '\n",
    "    'WHERE libelle_carburant is not null '\n",
    "    'GROUP BY libelle_carburant, dayofyear(date_releve)'\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une table \"day_price_df\" en Pandas pour pouvoir faire les graphes sous Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "graph = sns.FacetGrid(day_price_df[~day_price_df.libelle_carburant.isin(['E10','GPlc'])].sort_values(by=['libelle_carburant', 'day']),\n",
    "                      hue='libelle_carburant',\n",
    "                       height=6,aspect=16/9)\n",
    "graph.map(plt.plot, \"day\", \"day_price\")\n",
    "graph.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization(2/2)\n",
    "### Représentation de l'indice de prix par carburant et par département."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'a malheureusement pas été possible sur nos ordinateurs obsolètes d'exporter la table avec l'indice des prix par département sur Pandas. Cependant le code par la suite devrait permettre d'obtenir la représentation graphique demandée.\n",
    "Les cartes apparaissent dans le browsers par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exportation de table avec les departements et l'indice de prix sur pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_dep1=spark.sql(\n",
    "    'SELECT id_station, prix_ddf.libelle_carburant, departement, dayofyear(date_releve) as day, 100*((avg(prix)-daydep_price.daydep_price) / daydep_price.daydep_price + 1) as indice '\n",
    "    'FROM prix_ddf, daydep_price '\n",
    "    'WHERE prix_ddf.libelle_carburant is not null '\n",
    "        'and prix_ddf.libelle_carburant = daydep_price.libelle_carburant and dayofyear(date_releve) = daydep_price.day '\n",
    "    'GROUP BY id_station, prix_ddf.libelle_carburant, dayofyear(date_releve), daydep_price.daydep_price, departement'\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une liste à partir de la table pandas pour le carburant E10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E10_df = indice_dep1.loc[indice_dep1['libelle_carburant'] == 'E10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un dictionnaire qui associe le departement et l'indice de prix du carburant E10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E10_d=E10_df.set_index('departement')['indice'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "import pygal\n",
    "import lxml\n",
    "from pygal.maps.fr import Departments\n",
    "\n",
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price E10 by department'\n",
    "fr_chart.add('Average', E10_d)\n",
    "fr_chart.render_in_browser()\n",
    "svg(url='fr_chart.render()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SP98_df=indice_dep1.loc[indice_dep1['libelle_carburant'] == 'SP98']\n",
    "E85_df=indice_dep1.loc[indice_dep1['libelle_carburant'] == 'E85']\n",
    "Gazole_df=indice_dep1.loc[indice_dep1['libelle_carburant'] == 'Gazole']\n",
    "SP95_df=indice_dep1.loc[indice_dep1['libelle_carburant'] == 'SP95']\n",
    "GPLc_df=indice_dep1.loc[indice_dep1['libelle_carburant'] == 'GPLc']\n",
    "\n",
    "SP98_d=SP98_df.set_index('departement')['indice'].to_dict()\n",
    "E85_d=E85_df.set_index('departement')['indice'].to_dict()\n",
    "Gazole_d=Gazole_df.set_index('departement')['indice'].to_dict()\n",
    "SP95_d=SP95_df.set_index('departement')['indice'].to_dict()\n",
    "GPLc_d=GPLc_df.set_index('departement')['indice'].to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price SP98 by department'\n",
    "fr_chart.add('Average', SP98_d)\n",
    "fr_chart.render_in_browser()\n",
    "svg(url='fr_chart.render()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price E85 by department'\n",
    "fr_chart.add('Average', E85_d)\n",
    "fr_chart.render_in_browser()\n",
    "svg(url='fr_chart.render()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price Gazole by department'\n",
    "fr_chart.add('Average', Gazole_d)\n",
    "fr_chart.render_in_browser()\n",
    "svg(url='fr_chart.render()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price SP95 by department'\n",
    "fr_chart.add('Average', SP95_d)\n",
    "fr_chart.render_in_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price GPLc by department'\n",
    "fr_chart.add('Average', GPLc_d)\n",
    "fr_chart.render_in_browser()\n",
    "svg(url='fr_chart.render()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation du prix moyen par carburant et par département."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va représenter le prix moyen par carburant et par departement au lieu de l'indice de prix par departement. La table à convertir sous pandas est plus légère l'instruction fonctionne.\n",
    "A executer seulement si le code précédent n'a pas marché."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    'SELECT *,SUBSTRING(code_postal, 0, 2) as departement '\n",
    "    'FROM prix_ddf '\n",
    "    ).cache().createOrReplaceTempView('departement_df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a crée une table avec les departements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_price_df=spark.sql(\n",
    "    'SELECT libelle_carburant, round(avg(prix), 3) as depart_price, departement '\n",
    "    'FROM departement_df '\n",
    "    'WHERE libelle_carburant is not null '\n",
    "    'GROUP BY libelle_carburant, departement'\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E10_df=dep_price_df.loc[dep_price_df['libelle_carburant'] == 'E10']\n",
    "SP98_df=dep_price_df.loc[dep_price_df['libelle_carburant'] == 'SP98']\n",
    "E85_df=dep_price_df.loc[dep_price_df['libelle_carburant'] == 'E85']\n",
    "Gazole_df=dep_price_df.loc[dep_price_df['libelle_carburant'] == 'Gazole']\n",
    "SP95_df=dep_price_df.loc[dep_price_df['libelle_carburant'] == 'SP95']\n",
    "GPLc_df=dep_price_df.loc[dep_price_df['libelle_carburant'] == 'GPLc']\n",
    "\n",
    "E10_d=E10_df.set_index('departement')['depart_price'].to_dict()\n",
    "SP98_d=SP98_df.set_index('departement')['depart_price'].to_dict()\n",
    "E85_d=E85_df.set_index('departement')['depart_price'].to_dict()\n",
    "Gazole_d=Gazole_df.set_index('departement')['depart_price'].to_dict()\n",
    "SP95_d=SP95_df.set_index('departement')['depart_price'].to_dict()\n",
    "GPLc_d=GPLc_df.set_index('departement')['depart_price'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "import pygal\n",
    "import lxml\n",
    "from pygal.maps.fr import Departments\n",
    "\n",
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price E10 by department'\n",
    "fr_chart.add('Average', E10_d)\n",
    "fr_chart.render_in_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price SP98 by department'\n",
    "fr_chart.add('Average', SP98_d)\n",
    "fr_chart.render_in_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price E85 by department'\n",
    "fr_chart.add('Average', E85_d)\n",
    "fr_chart.render_in_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price Gazole by department'\n",
    "fr_chart.add('Average', Gazole_d)\n",
    "fr_chart.render_in_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price SP95 by department'\n",
    "fr_chart.add('Average', SP95_d)\n",
    "fr_chart.render_in_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_chart = pygal.maps.fr.Departments(human_readable=True)\n",
    "fr_chart.title = 'Average price GPLc by department'\n",
    "fr_chart.add('Average', GPLc_d)\n",
    "fr_chart.render_in_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling: Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PYTHONIOENCODING=\"UTF-8\"  \n",
    "\n",
    "services_ddf=(spark.read.load('./Services2018.csv',\n",
    "                              format='csv',sep='|',header='false',inferSchema='true')\n",
    "             .selectExpr(\n",
    "                 '_c0 as id_station',\n",
    "                 '_c1 as code_postal',\n",
    "                 '_c2 as type_station',\n",
    "                 '_c3 as latitude',\n",
    "                 '_c4 as longitude',\n",
    "                 '_c5 as services'\n",
    "            )\n",
    "             .dropna(subset=['services'])\n",
    "             .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "tronconneuse = RegexTokenizer(inputCol='services',outputCol='liste_services',pattern=',')\n",
    "#tronconneuse.transform(final_df1).show(5)\n",
    "vectorizer= CountVectorizer(inputCol='liste_services',outputCol='vecteur_services',binary=True)\n",
    "mon_pipeline=Pipeline(stages=[tronconneuse,vectorizer])\n",
    "vecteur_services_ddf=mon_pipeline.fit(services_ddf).transform(services_ddf).cache().createOrReplaceTempView('vecteur_services')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_pipeline.fit(services_ddf).transform(services_ddf).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_df =spark.sql('SELECT prix_table.id_station, prix_table.code_postal,prix_table.type_station,prix_table.Lat,prix_table.Long,prix_table.type_carburant,prix_table.libelle_carburant,prix_table.date_releve,year,month,week,prix, vecteur_services, dayofyear(date_releve) as day '\n",
    "         'FROM prix_table, vecteur_services '\n",
    "         'WHERE prix_table.id_station=vecteur_services.id_station ').cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "essai regression 1 OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['id_station', 'Long', 'Lat', 'type_carburant', 'day', 'vecteur_services'], outputCol = 'features')\n",
    "vfinal_df = vectorAssembler.transform(final_df)\n",
    "vfinal_df = vfinal_df.select(['features', 'prix'])\n",
    "vfinal_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vfinal_df.randomSplit([0.7, 0.3])\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]\n",
    "vfinal_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(labelCol='prix')\n",
    "lr_model = lr.fit(train_df.selectExpr('features','prix'))\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESSAI prediction 2 random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"prix\", outputCol=\"indexedLabel\").fit(vfinal_df)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "#featureIndexer = VectorIndexer(inputCol=['id_station', 'Long', 'Lat', 'type_carburant', 'day', 'vecteur_services'], outputCol=\"indexedFeatures\").fit(vfinal_df)\n",
    "    \n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = vfinal_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, rf, labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pipeline.fit(trainingData)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
